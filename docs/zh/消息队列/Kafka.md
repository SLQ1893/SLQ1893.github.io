## 1、Kafka 是什么？

官方解释：

> Apache Kafka is an open-source distributed event streaming platform used by thousands of companies for high-performance data pipelines, streaming analytics, data pipelines, streaming analytics, data integration, and mission-critical applications.

Kafka 是一个开源的分布式事件流处理平台，现在被数千家公司用于高性能数据管道、流分析、数据集成和一些关键的应用程序。

其实 Kafka 除了是一个开源的分布式流处理平台，还是一个开源的消息引擎系统，即：MQ，也可以作为分布式存储系统使用。

Kafka 最初由 Linkedin 公司开发，后来捐献给了 Apache 软件基金会成为顶级项目。

## 2、Kafka 有哪些优势？

##### Kafka 的核心能力有：

![](/images/Kafka/2.jpg)

##### 1、高吞吐量

可以在延迟低至 2 毫秒的机器集群，以网络有限的吞吐量传递消息。

##### 2、高可扩展

可在生产集群扩展多达上千个 broker、每天处理数万亿条消息、PB 级数据、数十万个分区，弹性扩展和收缩存储和处理。

##### 3、持久化

可以将数据流安全地存储在分布式、持久、容错的集群中。

##### 4、高可用性

可以在可用的分区上有效的扩展集群，还可以跨地理区域连接不同的分散集群。

## 3、Kafka 使用的是什么通信协议？

Kafka 的 Producer、Broker 和 Consumer 之间采用的是 Kafka 自行设计的一套简单、高性能、支持多语言基于 TCP 层的协议。

## 4、Kafka 支持哪几种消息模型？

##### Kafka 支持 2 种消息模型：

1、点对点模式（不可重复消费）

2、发布/订阅模式（可以重复消费）

## 5、Kafka 的服务端和客户端分别指什么？

服务端：Brokers

客户端：Producer（生产者）、Consumer（消费者）

## 6、Kafka 有哪些核心的组件？

##### 1、Topic

即：主题，是消息发布的对象，每一类别的消息可以分为一个 Topic（主题），实际开发中可以用来区分不同的业务系统。

##### 2、Producer

即：生产者，是指向某 Topic（主题）发布消息的客户端。

##### 3、Consumer

即：消费者，是指订阅某些 Topic（主题）的消息并进行消费的客户端。

##### 4、Brokers

Broker 是指 Kafka 的服务端，Kafka 集群由多个 Broker 组成，负责接收和处理客户端的请求，以及消息的持久化。

消费者可以订阅 1 个或多个 Topic（主题），并从 Broker 拉取已发布的消息进行消费。

## 7、Kafka 有哪几个核心的 API？

##### Kafka 有 4 个核心的 API：

##### 1、Producer API（生产者 API）

生产者可以发布消息到 1 个或多个 topic 主题中。

##### 2、Consumer API（消费者 API）

消费者可以订阅 1 个或多个 topic 主题，并处理消息。

##### 3、Streams API（流 API）

应用程序作为流处理器，从 1 个或多个 topic 主题中 消费输入流，然后生产 1 个输出流到 1 个或多个 topic 中，可以有效地将输入流转换输出流。

##### 4、Connector API（连接器 API）

可以构建并运行可以重用的生产者或消费者，并将 topic 主题连接到现有的应用程序或数据系统。

例如：可以连接到一个关系数据库，捕获表（table）的所有变更。

![](/images/Kafka/7.jpg)

## 8、Kafka 有哪些核心功能？

##### Kafka 的核心功能：

1、构建实时的流数据管道，可以在系统和应用程序之间的可靠地获取数据，即：MQ 消息队列。

2、构建实时流的应用程序，可以对数据流进行转换或影响，即：流处理。

## 9、Kafka 有哪些应用场景？

##### Kafka 的应用场景有：

1、消息队列（MQ）

2、流处理

3、日志收集

4、用户活动跟踪

5、指标

## 10、Kafka 为什么那么快？

##### 主要有以下几点原因：

1、顺序读写磁盘，避免了随机读写的性能问题；

2、利用了操作系统分页存储 Page Cache 来使用内存提高 I/O 效率。

3、利用了 Linux 的零拷贝技术，避免了昂贵的内核态数据拷贝；

4、对 Topic 分区，可以实现多台机器并行处理；

5、使用了批处理、压缩等优化手段传输数据，降低了网络 IO 开销；

## 11、Kafka 主要消耗什么资源？

##### 主要有以下三个资源方面的考量：

1、磁盘，因为 Kafka 需要用来存储消息；

2、内存，因为需要缓存磁盘上的部分数据；

3、带宽，因为需要通过网络传输大量数据；

## 12、Kafka 是有什么语言编写的？

Kafka 是由 Scala 和 Java 语言编写的，它们都是 JVM 系语言，所以 Kafka 编译后的源代码就是.class 文件。

## 13、Kafka 都有哪些发行版？如何选择？

目前主流的 Kafka 发行版本有以下几个：

##### 1、Apache Kafka（官方开源版本）

它是最早由 Apache 官方开源的社区版 Kafka，是最正宗的、用的最多的 Kafka 版本，它是其他所有 Kafka 发行版的基础。

##### 2、Cloudera/Hortnworks Kafka

它是一个集成了 Kafka 的大数据平台，全界面操作，十分友好，在安装部署、管理和监控方面要更加方便，省去了复杂的 Kafka 命令，但同时也降低了对 Kafka 的深度掌控。

##### 3、Confluent Kafka

他是一个专注于提供基于 Kafka 的企业级流处理的解决方案，目前分为免费版和企业版两种，它在社区版的基础上提供了如跨数据中心备份、Schema 注册中心等一系列高级特性。

一般的，用官方 Apache Kafka 就够了，也可以根据需要选择其他第三方版本。

## 14、Kafka 消息是推模式，还是拉模式？

Kafka 采用的是**拉模式（Pull）**,有客户端主动拉取消息消费。

## 15、Kafka 中 Zookeeper 的作用？

1、Zookeeper 存储了 Kafka 集群中的所有元数据信息，如 Broker 状态、Topic、分区数据等；

2、Kafka 中的 Leader 选举、扩容机制等也都靠 Zookeeper 完成；

## 16、Kafka 已经移除 Zookeeper 了吗？

是的，Kafka 2.8.0 版本已经移除了对 Zookeeper 的早期替换：

![](/images/Kafka/16.jpg)

但目前还没有彻底移除，默认还是使用 Zookeeper，但同时还可以使用最新的 KRaft 来管理 Kafka 集群，并且新增一个`@metadata`的内部 Topic 主题来存储元数据信息。

相信在后续的版本中，Kafka 会彻底移除对 Zookeeper 的依赖。

## 17、Kafka 为什么要移除 Zookeeper？

1、主要的原因是 Kafka 需要维护另外一个中间件，完全没有必要，还需要额外的运维成本；

2、另外，依赖 Zookeeper 会使得 Kafka 比较笨重，去 Zookeeper 可以完成自己的更轻量化、更优的性能；

## 18、Kafka 服务端实现高可用有哪些机制？

1、在不同的服务器上部署多个 Brokers；

2、备份机制，把数据备份到多台服务器上；

3、分区机制，对每个 Topic 拆分成多个分区；

## 19、Kafka Server 是什么网络模式？

Kafka 中的 SocketServer 是基于 Java NIO 开发的，采用的是 Reactor 模式。

包括一个 Acceptor 线程和网络线程池，Acceptor 线程主要负责接收客户端的请求，并使用轮询的方式分发到网络线程池中去处理。

## 20、Kafka 中的控制器是什么？

Kafka 中的控制器组件（Controller）也叫 Leader Broker，它是 Kafka Broker 集群中的一个 Broker，主要负责 Kafka 集群的管理和协调。

Kafka 同时只能有一个 Controller，如果当前 Controller 挂了会重新选举一个 Controller。

## 21、Kafka 中的控制器的作用有哪些？

##### Kafka 控制器的作用有：

1、集群 Broker 管理，新增 Broker、主动关闭 Broker 等；

2、Topic 管理，包括 Topic 的创建和删除，以及增加分区、分区重分配等；

3、Preferred 领导者选举；

4、向集群其他 Broker 提供数据服务；

## 22、Kafka 中的控制器是怎样产生的？

Kafka Broker 集群启动时，会通过选举机制选出其中一个 Broker 作为控制器，它们在启动时会尝试创建一个`/controller`的 Zookeeper 节点并写入自身信息，第一个完成节点创建的 Broker 就会被当选为当前 Kafka 集群的控制器。

## 23、Kafka 控制器是通过什么实现的？

Kafka 控制器是通过在 Zookeeper 中创建一个`/controller`临时节点实现的。

## 24、Kafka 控制器中保存了哪些数据？

##### Kafka 控制器中的一些重要数据：

1、所有 Broker 信息，包括当前运行中的 Broker、正在关闭中的 Broker 等；

2、所有 Topic 信息，包括具体分区信息、领导者副本信息、ISR 集合副本信息等；

3、当前进行中的 Preferred 领导者选举以及分区重分配的分区列表等。

## 25、Kafka 中的 ISR 是指什么？

ISR：In-sync-Replicas，即副本同步集合。

**ISR 里面放的是领导者副本，以及所有与领导者副本保持同步的追随者副本**，是否同步是根据 Broker 端 replica.lag.time.max.ms 参数值来确定的，如果追随者副本同步领导者副本的事件不超过这个值，就说明这个副本是保持同步到，否则就是不同步的。

并且，ISR 中的副本集合是会根据副本同步情况动态更新的。

## 26、Kafka 副本长时间不在 ISR 中说明什么？

这说明追随者副本不能及时同步领导者副本的进度，并且也不能参与正常的领导者选举，需要进行以下检查：

1、检查追随者副本所在的 Broker 与领导者副本的连接是否正常；

2、检查追随者副本所在的 Broker 的状态和负载情况是否正常；

## 27、Kafka 副本同步机制是怎样的？

Kafka 多个副本中，追随者副本会定期、异步地同步领导者副本上的消息数据。

## 28、Kafka 领导者副本挂了会怎样？

Kafka 领导者副本挂了会马上进行下一轮的 Leader 选举，从所有追随者副本中选出一个作为领导者副本。之前的领导者副本恢复后，就不再是领导者副本了，而是作为新领导者副本的追随者副本。

## 29、Kafka 中的 Unclean 选举是？

正常情况下，ISR 中的副本集合不应该是空的，选举应该从 ISR 集合中的副本中进行。但如果 ISR 副本集合为空，就只能从 ISR 副本集合之外的追随者副本中进行领导者副本选举，而这些副本可能远远落后于领导者副本，是不干净的副本，这就是 Unclean 选举。

Unclean 选举可以保证一直有领导者副本提供服务，但很大可能会造成数据丢失，所以不建议使用，建议使用其他高可用的替代方案。

## 30、Kafka 能避免消费者组重平衡吗？

计划性的增加 Consumer（消费者）可以不用避免。

但非自行的导致 Consumer（消费者）减少的情况可以通过设置合理的超时参数进行避免，比如 Consumer（消费者）被误认为已停止而被踢出消费者组：

1、没有及时发送心跳；

2、消费时间太长；

## 31、Kafka 中的分区是指什么？

Kafka 中的分区是指对 Topic（主题）进行分区，把一个主题拆分成多个分区。

![](/images/Kafka/31.jpg)

分区从 0 开始，生产的消息只能被写到其中一个分区中，每个分区都是有序且顺序不可变的记录集，并且可以持续追加，分区中的每条消息记录都会有一个唯一的 offset（偏移量）进行区分。

## 32、Kafka 为什么要进行分区？

##### 对 Topic 分区的好处是：

每个 Topic 可以不受单台服务器的限制，扩展性非常好，实现负载均衡，支持消费者并行消费， 提高吞吐量以处理更多的消息。

## 33、Kafka 分区策略有哪些？

常用的有以下 3 中分区策略：

##### 1、轮询策略

每条消息按顺序轮询写到 0~N 个分区。

##### 2、随即策略

每条消息随机写到 0~N 个分区。

##### 3、KEY 策略

KEY 相同的每条消息写到同一分区。

## 34、Kafka 分区策略默认是哪个？

如果消息指定了 KEY，则使用**KEY 策略**，KEY 相同的消息写到同一个分区。

如果消息没有指定 KEY，则默认使用**轮询策略**（最新版本）。

## 35、Kafka 数据备份副本分为哪几类？

##### Kafka 中的数据副本有 2 类：

1、领导者副本（Leader Replica），一个分区只有一个

2、追随者副本（Follower Replica），一个分区有 0 个或者多个

## 36、Kafka 副本和分区的关系是？

每个 Topic（主题）可以建立多个分区，而副本是对分区的拷贝，每个分区可以建立多个副本，可以有多个追随者副本（Follower Replica），但只能有一个领导者副本（Leader Replica）。

## 37、Kafka 数据副本都可以提供服务吗？

Kafka 中的数据副本只有**领导者副本（Leader Replica）**才能提供服务：

- 生产者向领导者副本（Leader Replica）写消息
- 消费者从领导者副本（Leader Replica）读消息

追随者副本（Follower Replica）不能提供服务，它只与领导者副本（Leader Replica）进行数据同步。

## 38、Kafka 支持读写分离吗？为什么？

Kafka 不支持读写分离，所有读写操作只能在领导者副本上进行。

主要是因为副本消息如果同步不及时，可能导致数据不一致性问题。

## 39、Kafka 创建 Topic 是否有数量限制？

Kafka 创建 Topic 的数量和 Topic 总分区数、每个 Topic 的分区数是有关的，Kafka 实例对 Topic 总分区数设置了上限，当达到上限后，会导致用户无法继续创建 Topic。

## 40、Kafka 创建 Topic 分区是否有数量限制？

Kafka 创建 Topic 分区有数量限制，当分区数达到上限后就无法再创建 Topic。

因为分区过多会导致消息生产、存储、消费都碎片化，影响性能和稳定性。

## 41、Kafka 可以减少 Topic 分区数吗？

Kafka 不支持减少 Topic 分区数，不然会导致数据丢失。

但可以通过删除原来的 Topic，然后创建新的 Topic 方式重新调整分区数。

## 42、Kafka 消费者与分区的对应关系是？

一个消费者可以消费多个分区的消息，但一个分区只能被一个消费者组中的一个消费者进行消费。

另外，如果一个 Topic（主题）只有一个分区，那么就只能有一个消费者进行消费。

## 43、Kafka 生产者是线程安全的吗？

Kafka 中的 KafkaProducer 生产者是线程安全的，多个线程可以共享一个 KafkaProducer 实例。

## 44、Kafka 消费者是单线程的吗？

Kafka 0.10.1.0 版本之前，Java Consumer 是单线程的设计。

Kafka 0.10.1.0+ 使用的是多线程设计，包括：

- 用户主线程：负责启动 Consumer 应用程序的主线程；
- 心跳线程：负责定期给对应的 Broker 机器发送心跳请求维持存活的线程；

但是获取消息仍是单线程的。

## 45、Kafka 消费者是线程安全的吗？

Kafka 中的 KafkaConsumer 消费者类不是线程安全的，不能在多线程中处理同一个 KafkaConsuer 实例，否则会抛出 ConcurrentModificationException 异常。

## 46、Kafka 会出现重复消费问题吗？

##### 可能会，有以下几种情况：

1、最主要的原因：消费时间过长，导致超过了 Kafka 的 session timeout 时间，那么此时就会触发重平衡，此时 offset 可能还没提交，会导致重平衡后重复消费。

2、强行 kill 消费者线程，导致消费后的数据 offset 没有提交（消费进行系统宕机、重启等）。

3、设置 offset 为自动提交，如果关闭 Kafka 在 close 之前，调用 consumer.unsubscribe()则有可能部分 offset 没提交，下次重启会导致重复消费。

## 47、Kafka 怎么避免重复消费问题？

1、尽量提高消费者端的消息处理能力，避免长时间处理超时而导致重平衡，如考虑采用异步、多线程的方式；

2、消费者端业务上使用幂等操作，避免消息重复处理；

## 48、Kafka 如何监控消费者的消费进度？

##### 主要有以下 3 种方法：

##### 1、使用 Kafka 中的 JMX 监控指标（推荐）；

2、使用 Kafka 中的 kafka-consumer-groups 命令行工具脚本；

3、使用 Kafka Java Consumer API 实现程序自动化监控；

## 49、Kafka 如何保证消费者线程安全性？

1、限制每个线程只能管理一个 KafkaConsumer 实例；

2、处理消息可以交给多个线程（线程池）去处理；

## 50、Kafka 消费者组是什么？

##### 消费者组是 Kafka 特有的概念。

消费者组是由多个消费者组成的一个组，消费者组可以消费订阅的一组 Topic（主题 ），即该组 Topic（主题）中的消息只能被该消费者组中的一个消费者进行消费，消费者组之外的其他消费者不能消费。

如图：

![](/images/Kafka/50.jpg)

Kafka 集群有两个 Broker 服务器，每个服务器都有两个分区，它们都属于同一个 Topic（主题），分区中的消息由该消费者组进行瓜分。

## 51、Kafka 为什么引入消费者组？

##### Kafka 通过消费者组：

1、可以提高消费者端的消费能力；

2、可以同时支持点对点模式和发布订阅模式；

## 52、Kafka 消费者组如何分配消费者数量？

如图所示：

![](/images/Kafka/52.jpg)

消费者数量最好设置成所有 Topic 的总分区的数量，或者总分区数是消费者数的倍数。

1、如果上图有 6 个消费者，超过数量的 2 个消费者不会分配分区。

2、如果上图有 3 个消费者，那就只有 2 个消费者会分配分区，另外一个消费者不会进行分配。

## 53、Kafka 中的重平衡是指什么？

Kafka 中的重平衡（Rebalance）机制是建立在消费者组之上的，它规定了如何让消费者组内的消费者分配 Topic 中的每一个分区。

##### 比如：

一个主题有 100 个分区，一个消费者组有 10 个消费者，重平衡就是如何让这 10 个消费者各自分配到 10 个分区的过程，并且，如果其中一个消费者中途挂了，Kafka 会自动检测，并重新把失败的分区分配给其他正常的消费者。

## 54、Kafka 什么时候会触发重平衡？

##### 有以下 3 种情况：

##### 1、消费者组内成员发生变更，包括消费者的增加和减少（大部分是这种情况）；

2、订阅的 Topic 主题发生变更，如：新建了对应订阅规则的主题；

3、对应的 Topic 主题分区数发生变更，如：增加了分区；

## 55、Kafka 中的重平衡有什么弊端吗？

Kafka 中的重平衡有性能影响，对 TPS 的影响非常大，所以应该尽量避免发生重平衡。

1、重平衡过程中**Kafka 基本处于不可用状态**，这个时段消费者是无法消费消息的；

2、如果消费者组中的消费者很多，那重平衡会很慢，甚至几个小时之久。。

## 56、Kafka 中的偏移量有什么用？

##### Kafka 中的偏移量有两种概念：

1、用来区分分区中的每条消息记录，它是唯一、顺序增长的。

2、每个消费者会保存消费的偏移量值，用来标识它消费的位置，当消费消息的时候，偏移量会进行线性的增加，偏移量由消费者进行控制的，所以消费者可以将偏移量重置到之前的位置以重新消费消息，也可以跳过一些消息消费。

![](/images/Kafka/56.jpg)

## 57、Kafka 中的 Lag 指标是指什么？

在 Kafka 中，Lag 是**“滞后”、“延迟”**的意思，Lag 的值为**正数、负数、0**分别代表着不同的指标，需要监控消费者的消费进度，以免业务受影响。

##### 正数：

正数则表示生产者消息堆积、积压，数字越大，说明消费者远远跟不上生产者的速度，这样就可能导致性能问题。

##### 负数：

负数则表示消费者的消费速度很快，远远超过了生产者的速度，供小于求。

##### 0：

0 表示生产者和消费者的速率都处于正常水平，说明生产者和消费者都工作正常。

## 58、Kafka 支持消息顺序性？

Kafka 只支持 Topic 下单个分区的消息顺序性。

## 59、Kafka 支持优先队列吗？

Apache Kafka**不支持**优先队列。

## 60、Kafka 支持死信队列吗？

Apache Kafka**不支持**死信队列。

## 61、Kafka 支持延迟队列吗？

Apache Kafka**不支持**延迟队列。

## 62、Kafka 消息堆积了怎么办？

可以在 Kafka 监控平台（Kafka Manager）上找出该消息堆积的消费者组，如果该消费者组有消费者在消费，则想办法加快消费者的消费效率，如果没有，可以考虑移除不使用的消费者组。

## 63、Kafka 如何保证消息的顺序性？

##### 一版有以下两种做法：

1、设置需要顺序消费的 Topic 只分一个区（分一个区影响吞吐量，这种不推荐）；

2、设置需要顺序消费的消息按 KEY 策略保存到一个分区，然后消费者数等于分区数，每个消费者消费一个分区，保证了消息的顺序性；

## 64、Kafka 生产消息的最大长度是多少？

Kafka 生产消息的最大长度为：10MB。

## 65、Kafka 中的高水位是指什么？

Kafka 中的高水位（**High Watermark**），缩写：**HW**，它是一个用消息位移来表示的位置信息标记，比如某个副本中 HW=10，就表明这个副本的高水位在 offset=10 这个位置。

## 66、Kafka 中的高水位有什么用？

##### Kafka 中的高水位的作用主要有：

1、用来标识分区下的消息哪些是可以被消费的，高水位以下的消息是已提交消息，高水位以上（>=）的是未提交消息（不能被消费）；

2、完成副本同步；

## 67、Kafka 中的高水位会带来什么问题？

Kafka 是使用高水位值（HW）来决定副本备份的进度，而 HW 值往往需要通过额外一轮 FETCH RPC 才能完成更新，所以 HW 值的是有延迟的，可能会引起以下问题：

- 备份数据丢失
- 备份数据不一致

## 68、Kafka 是如何解决高水位引起的问题的？

Kafka 0.11 开始引入了一种**Leader Epoch**机制来取代高水位。

##### Leader Epoch 信息包含一对值：

- epoch：表示的是 Leader 副本版本号，从 0 开始，每次 Leader 变更+1；
- offset：表示的是该 Epoch 版本的 Leader 副本写入第一条消息的位移；

有了 Leader Epoch 信息，就可以避免高水位导致的数据丢失及不一致的情况。

## 69、Kafka 可以读取失败的事务消息吗？

可以，因为 Consumer（消费者）默认是读取**read_uncommitted**级别的消息，即可以读取任何消息。

可以将 Consumer（消费者）设置为**read_committed**，只读取成功事务的消息和其他所有非事务的消息。

## 70、Kafka 支持事务消息吗？

Kafka 从 0.11 版本开始，Producer 可以支持事务消息，当 Producer 开启事务后，写入一批消息，要么全部成功，要么全部失败。

## 71、Kafka 支持消息幂等性吗？

Kafka 支持单个生产者、单个分区、单个会话的消息幂等性。

## 72、Kafka 中的 Producer 是幂等性的吗？

Kafka 中的 Producer（生产者）默认不是幂等性的，但可以通过参数`enable.idempotenct=true`设置成幂等性。

## 73、Kafka 会丢失消息吗？

有可能，Broker、生产者、消费者都有可能丢失数据。

比如：网络异常、客户端异常、Leader 副本异常、缓冲区满了。

## 74、Kafka 怎么解决消息重复发送问题？

我们可以将 Producer（生产者）设置成幂等性，这样 Kafka 就可以自动帮我们去除重复发送的消息。

> 注意：这个设置仅限于单个分区、以及所在的会话，不能跨分区、跨会话实现幂等性去重。

## 75、Kafka 中的消息是永久存储的吗？

不是，Kafka 不会一直保留数据，也不会等到消息被消费之后才删除消息，Kafka 可以为每个主题配置了数据保留策略，达到指定的策略就会删除数据。

## 76、Kafka 有哪几种数据保留的策略？

Kafka 有以下两种数据保存策略：

1、按照设置的消息的过期时间保留；

2、按照设置的存储的消息大小保留；

## 77、Kafka 消息压缩、解压的过程？

Kafka 的消息压缩主要用在**Producer（生产者）**端，用来提高网络 IO 的传输效率。

正常情况下，Broker 不会重新解压再重新压缩，除非 Producer 使用了和 Broker 不同的压缩算法（以 Broker 的为准），或者 Broker 端需要做消息格式转换以兼容保存多种格式的消息。

**Consumer（消费者）端**消费消息时，需要根据消息集合中保存的压缩算法进行解压。

## 78、Kafka 支持哪几种压缩算法？

##### Kafka 目前支持以下几种压缩算法：

- GZIP
- Snappy
- LZ4
- Zstandard/或者 zstd（Kafka 2.1.0+）

来看 zstd 给出的性能测试：

| Compressor name          | Ratio | Compression | Decompress |
| ------------------------ | ----- | ----------- | ---------- |
| zstd 1.4.5 - 1           | 2.884 | 500MB/s     | 1660MB/s   |
| zlib 1.2.11 - 1          | 2.743 | 90MB/s      | 400MB/s    |
| brotli 1.0.7 - 0         | 2.703 | 400MB/s     | 450MB/s    |
| **zstd 1.4.5 -- fast=1** | 2.434 | 570MB/s     | 2200MB/s   |
| **zstd 1.4.5 -- fast=3** | 2.312 | 640MB/s     | 2300MB/s   |
| quicklz 1.5.0 - 1        | 2.238 | 560MB/s     | 710MB/s    |
| **zstd 1.4.5 -- fast=5** | 2.178 | 700MB/s     | 2420MB/s   |
| lzo1x 2.10 - 1           | 2.106 | 690MB/s     | 820MB/s    |
| lz4 1.9.2                | 2.101 | 740MB/s     | 4530MB/s   |
| **zstd 1.4.5 -- fast=7** | 2.096 | 750MB/s     | 2480MB/s   |
| lzf 3.6 - 1              | 2.077 | 410MB/s     | 860MB/s    |
| snappy 1.1.8             | 2.073 | 560MB/s     | 1790MB/s   |

zstd 压缩能力最强，lz4 解压能力最强。

## 79、Kafka 集群管理工具使用的哪个？

目前用的最多的是 Yahoo 开源的 Kafka Manager，它是一个基于 Web 的管理工具。可能怕误认为是 Kafka 的官方工具，现在已经改名叫：CMAK（Cluster Manager for Apache Kafka）。

> https://github.com/yahoo/CMAK

## 80、Kafka 如何选择合适的磁盘？

Kafka 使用普通的机械硬盘就可以了。

因为 Kafka 使用的是磁盘顺序 IO 读写，不需要用到随机 IO 读写，所以虽然 SSD 在随机 IO 读写性能比较好，但在 Kafka 上面完全没有必要。

当然，有钱任性，使用 SSD 肯定是更好的，但一般情况下没有必要。

## 81、Kafka 如何设置堆的大小？

一般情况下设置 6G~8G 左右就够了，也可以查看 GC 日志，在 Full GC 后，设置为存活对象总大小的 1.5 到 2 倍。

## 82、Kafka 如何选择合适的 GC 回收器？

建议使用 G1 垃圾回收器，G1 的优化难度要比 CMS 收集器小很多，G1 从 JDK9 开始就已经成为 JVM 中的默认垃圾回收器了。

## 83、Kafka Streams 是什么？

Kafka Streams 是用于构建分布式应用程序和微服务的 Java 客户端库，其中输入和输出数据存储在 Kafka 集群中。

##### Kafka Streams 的特性：

- 高弹性、高扩展性、容错性
- 部署到容器、虚拟机、裸机、云端
- 支持小型、中型和大型应用
- 与 Kafka 安全、无缝集成
- 编写标准的 Java 和 Scala 应用程序
- 一次处理语义
- 不需要单独的处理集群

## 84、什么是流处理平台？

先了解什么是**数据流**，它也叫事件流，是无边界数据集的抽象表示，随着时间的推移，性的数据记录会不断加入进来，所以无边界意味着数据是无限和持续增长的。

流式处理平台，则是指持续的从一个无边界的数据集读取数据，然后对它们进行处理并生成结果。

## 85、流处理平台有哪些功能特性？

##### 流处理平台有以下三种特性：

1、可以发布和订阅流式记录，类似于消息队列系统；

2、可以存储流式数据记录，并且有较好的容错性；

3、可以在流式数据记录产生时就进行处理；

## 86、如何搭建一个实时流处理平台？

##### 可以使用以下技术栈：

1、Flume + Kafka + Flink；

2、Kafka COnnect + Kafka Core + Kafka Streams；
